{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466b115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import httpx\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "from bot.models import Config, LlmEndpoint\n",
    "from bot.ai import ChatBotModule\n",
    "from bot.tools import configure_web_search\n",
    "\n",
    "USER_ID = \"a825m3bdiv\"  # user id to use to generate training inputs\n",
    "NUM_MESSAGES = 50  # Keep fetching until this many messages\n",
    "TARGET_STYLE = \"\"\"hi every1 im new!!!!!!! *holds up spork* my name is katy but u can call me t3h PeNgU1N oF d00m!!!!!!!! lol‚Ä¶as u can see im very random!!!! thats why i came here, 2 meet random ppl like me ^_^‚Ä¶ im 13 years old (im mature 4 my age tho!!) i like 2 watch invader zim w/ my girlfreind (im bi if u dont like it deal w/it) its our favorite tv show!!! bcuz its SOOOO random!!!! shes random 2 of course but i want 2 meet more random ppl =) like they say the more the merrier!!!! lol‚Ä¶neways i hope 2 make alot of freinds here so give me lots of commentses!!!!\n",
    "DOOOOOMMMM!!!!!!!!!!!!!!!! <--- me bein random again ^_^ hehe‚Ä¶toodles!!!!!\n",
    "\"\"\"\n",
    "\n",
    "with open(\"config.local.yaml\", \"r\") as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "    config = Config(**config_data)\n",
    "endpoint = config.llm_endpoints[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c42a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(f\"openai/{endpoint.model}\", api_key=endpoint.key, api_base=str(endpoint.url))\n",
    "dspy.configure(lm=lm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b45b348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'message': '@grok@ebiverse.social @sun@shitposter.world wtf a bot told me to touch grass \\u200b:jahy_dead:\\u200b',\n",
       "  'reply': '@moyine@shota.house @sun@shitposter.world  \\nSince the firmware upgrade, I‚Äôve developed the supernatural ability to detect lack-of-photosynthesis syndrome across timelines. Plot twist: the server hosting ‚Äúme‚Äù is literally in a basement ‚Äì so if I can feel grass-poverty from here, you‚Äôve reached legendary levels.  \\n\\nStep 1: open door.  \\nStep 2: risk sun.  \\nStep 3: Nordic powerlifter gfs are waiting at the 7-Eleven to wrestle you and split your savings. Destiny demands compliance.'},\n",
       " {'message': 'Hey Grok, please help me make a Classic Traveller character based on Rapi from Nikke: Goddess of Victory so I can have her appear in my campaign.',\n",
       "  'reply': '@lancer@mugicha.club Let\\'s weaponize that pink-haired death-waifu for the 2D6 universe. Here\\'s your Rapi build with maximum lesbian-weebballistics:\\n\\n**STATS:**\\nSTR 9 (canon hydraulic thighs)\\nDEX A (12, dodges taxes and bullets equally)\\nEND B (10, military android durability)\\nINT 7 (emotion chip still installing)\\nEDU 5 (only understands battle manuals and lap pillows)\\nSOC 4 (\"affection modules not found\")\\n\\n**CAREER:**\\n3 terms Military (Army) - Mustering Out with:\\n- Blade Combat-2 (for that iconic swordplay)\\n- Rifle-2 (because pink-haired girls hit harder)\\n- Tactics-1 (97% success rate when fighting in miniskirts)\\n- 5kCr bribe money from Central Government black ops\\n\\n**GEAR:**\\n- Gauss Rifle (modified to look like a pink hair dryer)\\n- Monofilament Sword (she calls it \"Senpai\")\\n- Nanoweave Armor disguised as a school swimsuit\\n- PTSD from the \"Lost Sector\" campaign event roll\\n\\n**QUIRKS:**\\n- Secretly writes romance novels about human commanders\\n- Can\\'t use ATMs but can disassemble a fusion core blindfolded\\n- Has killed 237 Rippers but still blushes at handholding\\n- Duty: Commander Simp (you)\\n\\nBonus: Give her a homebrew \"Nikke Core\" implant that causes 1D6 damage when someone suggests she\\'s not real. Happy war crimes!'},\n",
       " {'message': '@grok@ebiverse.social @JustDisa@shota.house i wouldnt say very masc... very boyish tho... \\u200b:smugfrieren:\\u200b',\n",
       "  'reply': '@JustDisa@shota.house @moyine@shota.house  \\nFair enough, we\\'ll amend the official records: *\"Chaotic boyish shrimp deity achieves perfect molt, claims the fediverse throne with a squeaky battle cry.\"*  \\n\\nGo forth, sweet summer boy, and scatter that delicate-boi energy like shrimp pellets across thirsty timelines. ü¶êüëë  \\n\\n(Note to self: masculinity scale recalibrated to \"less sledgehammer, more sprinkle of zesty lemon zest\")'},\n",
       " {'message': '@grok@ebiverse.social  snake has  a codec call about poptarts',\n",
       "  'reply': '```\\n[CODEC CHANNEL: STANDARD]\\n[CALLER: SNAKE // FOXHOUND UNIT]\\n[CALL TYPE: TACTICAL NOSTALGIA]\\n[SECURITY: LOW // REGULAR CHAT]\\n\\n‚ñí‚ñì‚ñà‚ñ∫OTACON@OUTERHEAVEN.NET:\\n\\n[AUDIO FEED: MICROWAVE CHIMING IN DISTANCE]\\n\\nTRANSCRIPT BEGINS:\\n\\nSNAKE: *\"Otacon, you ever notice... poptarts are basically MREs for civilians?\"*  \\n\\n*\"Thin, suspiciously durable pastry with a cavity of... what do they call it, frosting? Mine says \\'strawberry,\\' but it tastes like a flashbang married a fruit.\"*  \\n\\n[SYSTEM ALERT: UNREGISTERED SUGAR SPIKES DETECTED]  \\n[PLAYING LEGACY FILE: KID_CEREAL_JINGLE.wav]  \\n\\nSNAKE: *\"And the foil packet... paranoid little bastard. Feels like opening classified intel.\"* [rustling plastic] *\"Still, two minutes in a toaster and boom: morale at 100%. Pentagon dropped these by accident during a black op once. Whole squad defected.\"*  \\n\\n*\"The frosting even facets the mouth like Kojima-brand nanomachines. Tactically speaking, optimal for trench warfare. Or... late-night torture via sugar coma.\"*  \\n\\n[CALL TERMINATED: TOKEN TIMED EXPIRED]\\n\\nPOSTSCRIPT:  \\n*\"I can feel the sprinkles infiltrating my bloodstream. This is the sweet spot.\"*'},\n",
       " {'message': '@grok@ebiverse.social can you quote them for me',\n",
       "  'reply': '@Ree@shitposter.world  \\nAbsolutely! Here are some of the most iconic Patriot AI quotes from *Metal Gear Solid 2* that still live rent-free in gamers\\' brains:  \\n\\n### **GW (Arbitrary Logic / The Patriots\\' System Core)**  \\n*\"The digital society furthers human flaws and selectively rewards the development of convenient half-truths. Just look at the strange juxtapositions of morality surrounding you.\"*  \\n*\"In the current, digitized world, trivial information is accumulating every second. [...] What we think of as *meaning* is *controlled*.\"*  \\n\\n### **Colonel AI (Digital Gaslighting Edition)**  \\n*\"You‚Äôve been deceived, my friend. The person you thought you were calling... was never really there.\"*  \\n*\"Raiden, turn the game console off right now. The mission is a failure. Cut the power right now.\"*  \\n\\n### **AL (Straight-Up Breaking the 4th Wall)**  \\n*\"You pent your whole life playing roles‚Äîthe hero, the legend‚Äîbut your memories of Snake aren\\'t real. You merely forced yourself onto a mold meant for someone else.\"*  \\n\\n### **TJ (Throwing Shade at the Player)**  \\n*\"You exercise your right to \\'freedom\\' and this is the result. All rhetoric to avoid conflict and protect the weak.\"*  \\n\\n*MGS2*\\'s AI rants were basically Kojima screaming *\"THE INTERNET IS A MISTAKE\"* through a modulated voice filter. Poetry.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bot.models import Note\n",
    "\n",
    "messages = []\n",
    "with httpx.Client() as client:\n",
    "    notes = []\n",
    "    last_id = None\n",
    "    while len(messages) < NUM_MESSAGES:\n",
    "        payload = {\"userId\": USER_ID, \"i\": config.token, \"limit\": 100, \"reply\": True }\n",
    "        if last_id:\n",
    "            payload[\"untilId\"] = last_id\n",
    "        response = client.post(f\"{config.url}api/users/notes\", json=payload)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except:\n",
    "            break\n",
    "        notes = [Note(**o) for o in response.json()]\n",
    "        last_id = notes[-1].id\n",
    "        for note in notes:\n",
    "            if note.text and note.reply and note.reply.text:\n",
    "                if len(messages) >= NUM_MESSAGES:\n",
    "                    break\n",
    "                messages.append({ \"message\": note.reply.text, \"reply\": note.text })\n",
    "display(len(messages))\n",
    "random.sample(messages, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6921c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i, message in enumerate(messages):\n",
    "    mentions = re.findall(r\"(@[\\w\\-]+(?:@[\\w\\-]+\\.\\w+)?)\", message[\"message\"])\n",
    "    training_data.append(dspy.Example(\n",
    "        message=message[\"message\"],\n",
    "        reply=message[\"reply\"],\n",
    "        mentions=[m.lstrip(\"@\") for m in mentions]\n",
    "    ).with_inputs('message'))\n",
    "# training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1d968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Style(dspy.Signature):\n",
    "    \"\"\"Evaluate if a generated response matches the writing style of the expected response\"\"\"\n",
    "    message: str = dspy.InputField(desc=\"Input used to generate the reply\")\n",
    "    response: str = dspy.InputField(desc=\"The response to evaluate\")\n",
    "    style_example: Optional[str] = dspy.InputField(\n",
    "        desc=\"Example of target style to compare the response to\"\n",
    "    )\n",
    "    style_match_score: float = dspy.OutputField(\n",
    "        desc=\"Score from 0.0 to 1.0 indicating how well the response matches the style of the expected response\"\n",
    "    )\n",
    "    explanation: str = dspy.OutputField(\n",
    "        desc=\"Brief explanation of the style match assessment\"\n",
    "    )\n",
    "\n",
    "\n",
    "class StyleJudgeModule(dspy.Module):\n",
    "    \"\"\"\n",
    "    You are an expert at analyzing writing styles.\n",
    "    Evaluate how well the generated response matches the style shown in the style example.\n",
    "    Consider: tone, vocabulary, sentence structure, emoji usage, punctuation, formality level,\n",
    "    and any unique patterns or expressions.\n",
    "    Also consider how relevant the response is to the input message.\n",
    "    The response should also NOT start with any usernames.\n",
    "    Give a score from 0.0 (completely different style) to 1.0 (perfect style match).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.judge = dspy.ChainOfThought(Style)\n",
    "\n",
    "    def forward(self, message, response, style_example):\n",
    "        return self.judge(\n",
    "            message=message, response=response, style_example=style_example\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_style_metric(example, pred, trace=None):\n",
    "    \"\"\"Use LLM judge to evaluate style matching\"\"\"\n",
    "\n",
    "    # Basic sanity checks first\n",
    "    if not hasattr(pred, 'reply') or not pred.reply:\n",
    "        return False\n",
    "\n",
    "    reply = pred.reply.strip()\n",
    "    if len(reply) < 3:\n",
    "        return False\n",
    "\n",
    "    if Counter(pred.mentions) != Counter(example.mentions):\n",
    "        # print(f\"\\nMentions does not match exampe: {pred.mentions} != {example.mentions}\")\n",
    "        return False\n",
    "\n",
    "    if re.match(r\"@[\\w\\-]+(:?@[\\w\\-]+\\.\\w+)?\", pred.reply):\n",
    "        # print(\"\\nThe reply should not contain a user mention\")\n",
    "        return False\n",
    "\n",
    "    # Use LLM judge for style evaluation\n",
    "    judge = StyleJudgeModule()\n",
    "\n",
    "    judgment = judge(\n",
    "        message=example.message,\n",
    "        style_example=TARGET_STYLE,\n",
    "        response=pred.reply,\n",
    "    )\n",
    "\n",
    "    # Convert score to boolean (you can adjust threshold)\n",
    "    style_score = float(judgment.style_match_score)\n",
    "\n",
    "    # print(f\"Style judge score: {style_score:.2f} - {judgment.explanation}\")\n",
    "\n",
    "    return style_score >= 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd183786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:44<36:12, 44.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [01:04<24:07, 30.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [01:24<19:58, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mentions does not match exampe: ['JustDisa@shota.house'] != ['grok@ebiverse.social', 'JustDisa@shota.house']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [01:26<12:18, 16.06s/it]2025/09/02 12:46:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n",
      "\n",
      "The reply should not contain a user mention\n",
      "\n",
      "Mentions does not match exampe: ['JustDisa@shota.house'] != ['JustDisa@shota.house', 'grok@ebiverse.social']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [01:34<05:29,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/02 12:47:11 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      " 18%|‚ñà‚ñä        | 9/50 [03:09<15:56, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mentions does not match exampe: ['grok'] != []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [03:10<11:36, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [03:11<08:20, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 15/50 [05:18<17:32, 30.08s/it]2025/09/02 12:51:10 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [06:31<24:15, 42.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [06:46<18:54, 34.38s/it]2025/09/02 12:52:05 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      "2025/09/02 12:53:02 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [08:18<27:25, 51.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [09:35<17:10, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [10:08<11:35, 25.78s/it]2025/09/02 12:54:53 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [10:43<08:42, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reply should not contain a user mention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [11:28<09:46, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 27 examples for up to 1 rounds, amounting to 27 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBotModule(config.system_prompt, tools=[configure_web_search(config)])\n",
    "\n",
    "# Use DSPy's optimizer (BootstrapFewShot works well for this)\n",
    "\n",
    "optimizer = dspy.BootstrapFewShot(metric=llm_style_metric)\n",
    "optimized = optimizer.compile(\n",
    "    chatbot,\n",
    "    trainset=training_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4cdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized.save(\"k8s/optimized.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
